{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a55556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import os\n",
    "import timm\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import F1Score\n",
    "import ttach as tta\n",
    "import wandb\n",
    "from sklearn.utils import class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available else \"cpu\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2665c870",
   "metadata": {},
   "source": [
    "# Split data to train val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8bd36b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46514481-2d8b-4d49-8991-012e1bfd34f6.jpeg</td>\n",
       "      <td>swimming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ec66e513-adac-4a30-b6a9-3d647ee6e46b.jpeg</td>\n",
       "      <td>greco-Roman_wrestling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4d60732e-d680-4bfd-9067-70ff8137f537.jpeg</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93327011-8e3d-4f0d-849d-a26ddaf6488b.jpeg</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b6853478-48c1-48b2-b104-74903730c831.jpeg</td>\n",
       "      <td>sailing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image_id                  label\n",
       "0  46514481-2d8b-4d49-8991-012e1bfd34f6.jpeg               swimming\n",
       "1  ec66e513-adac-4a30-b6a9-3d647ee6e46b.jpeg  greco-Roman_wrestling\n",
       "2  4d60732e-d680-4bfd-9067-70ff8137f537.jpeg                running\n",
       "3  93327011-8e3d-4f0d-849d-a26ddaf6488b.jpeg               football\n",
       "4  b6853478-48c1-48b2-b104-74903730c831.jpeg                sailing"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "# split data on train and validation \n",
    "X = df[\"image_id\"].values\n",
    "y = df[\"label\"].values\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, \n",
    "                                                  y,\n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=42,\n",
    "                                                  stratify=y, \n",
    "                                                  shuffle=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95c6ef",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "655c5ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "X_test = test_data[\"image_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f0a47",
   "metadata": {},
   "source": [
    "# ToDo  \n",
    "* weighted CE +\n",
    "* full training pipeline from config\n",
    "* wandb logger\n",
    "* weights saving\n",
    "* proper model freezing (like from seminar) +\n",
    "* TTA ++\n",
    "* func for subm + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0618db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode sport names to digits\n",
    "sport_to_idx = dict(zip(np.unique(df[\"label\"].values), \n",
    "                        [i for i in range(len(np.unique(df[\"label\"].values)))]))\n",
    "idx_to_sport = dict(zip([i for i in range(len(np.unique(df[\"label\"].values)))],\n",
    "                        np.unique(df[\"label\"].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68bc7acc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes = df[\"label\"].unique()\n",
    "class_weights = class_weight.compute_class_weight(class_weight=\"balanced\",\n",
    "                                                classes=classes,\n",
    "                                                y=df[\"label\"].values)\n",
    "class_weights = dict(zip(classes, class_weights))\n",
    "class_weights = {sport_to_idx[sport] : class_weights[sport] for sport in class_weights}\n",
    "class_weights = np.array(sorted(class_weights.items(), key=lambda x: x[0]))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08e0e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SportsDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 path_to_imgs, \n",
    "                 img_list, \n",
    "                 label_list,\n",
    "                 sport_dict, \n",
    "                 is_test=False,\n",
    "                 transforms=None):\n",
    "        \n",
    "        self.path_to_imgs = path_to_imgs\n",
    "        self.image_list = img_list\n",
    "        self.label_list = label_list\n",
    "        self.sport_dict = sport_dict\n",
    "        self.is_test = is_test\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_list[idx]\n",
    "        image = Image.open(os.path.join(self.path_to_imgs, img_name)).convert(\"RGB\")\n",
    "        if self.is_test:\n",
    "            if self.transforms:\n",
    "                image = self.transforms(image)\n",
    "            return image\n",
    "        label = self.label_list[idx]\n",
    "        encoded_label = self.sport_dict[label]\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        return image, encoded_label\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73811a4d",
   "metadata": {},
   "source": [
    "# Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb9ba09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = T.Compose([T.RandomRotation(degrees=90),\n",
    "                              T.RandomVerticalFlip(),\n",
    "                              T.RandomHorizontalFlip(),\n",
    "                              T.ToTensor(),\n",
    "                              T.Resize((384, 384)),\n",
    "                              T.RandomErasing(),\n",
    "                              T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                    std=[0.229, 0.224, 0.225]),\n",
    "                             ])\n",
    "tta_tf = tta.Compose([tta.HorizontalFlip(), \n",
    "                      tta.Rotate90([0, 90, 180, 270]), \n",
    "#                       tta.Multiply(factors=[0.9, 1, 1.1]),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a79914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_transforms = T.Compose([T.ToTensor(), \n",
    "                        T.Resize((224, 224)),\n",
    "                        T.Normalize(mean=[0.5, 0.5, 0.5], \n",
    "                                    std=[0.5, 0.5, 0.5])]) # from model default cfg\n",
    "# mb add flips to train part\n",
    "val_transforms = simple_transforms\n",
    "train_dataset = SportsDataset(path_to_imgs=\"train/\",\n",
    "                              img_list=X_train,\n",
    "                              label_list=y_train,\n",
    "                              sport_dict=sport_to_idx,\n",
    "                              is_test=False,\n",
    "                              transforms=simple_transforms)\n",
    "\n",
    "val_dataset = SportsDataset(path_to_imgs=\"train/\",\n",
    "                            img_list=X_val,\n",
    "                            label_list=y_val,\n",
    "                            sport_dict=sport_to_idx, \n",
    "                            is_test=False,\n",
    "                            transforms=val_transforms)\n",
    "\n",
    "test_dataset = SportsDataset(path_to_imgs=\"test/\",\n",
    "                             img_list=X_test,\n",
    "                             label_list=None,\n",
    "                             sport_dict=sport_to_idx,\n",
    "                             is_test=True,\n",
    "                             transforms=val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e4beb",
   "metadata": {},
   "source": [
    "Did it once in order to compute statistics\n",
    "\n",
    "Train mean R: 0.0, G: 0.002, B: -0.001  \n",
    "Train std R: 1.002, G: 0.999, B: 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5ff6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835da436",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bff91e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_training(model, mode):\n",
    "    # disable traning for all layers\n",
    "    model.eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "#     for p in model.stages[3].parameters():\n",
    "#         print(p.requires_grad) # double check\n",
    "    if mode == \"stage3\":\n",
    "        model.stages[3].train()\n",
    "        for p in model.stages[3].parameters():\n",
    "            p.requires_grad = True\n",
    "    #         print(p.requires_grad) # double check\n",
    "    elif mode == \"stage3_block2\":\n",
    "        model.stages[3].blocks[2].train()\n",
    "        for p in model.stages[3].blocks[2].parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    elif mode == \"stage2\":\n",
    "        model.stages[2].train()\n",
    "        for p in model.stages[3].parameters():\n",
    "            p.requires_grad = True\n",
    "        model.stages[2].train()\n",
    "        for p in model.stages[3].parameters():\n",
    "            p.requires_grad = True\n",
    "    elif mode == \"vit_block31\":\n",
    "        model.blocks[31].train()\n",
    "        for p in model.blocks[31].parameters():\n",
    "            p.requires_grad = True\n",
    "    elif mode == \"beit_block23\":\n",
    "        model.blocks[23].train()\n",
    "        for p in model.blocks[23].parameters():\n",
    "            p.requires_grad = True\n",
    "            \n",
    "    model.head.train()\n",
    "    for p in model.head.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22c038b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submit(model, \n",
    "                  test_loader, \n",
    "                  label_mapper,\n",
    "                  experiment_name,\n",
    "                  path_to_test_csv=\"test.csv\",\n",
    "                  device=device,\n",
    "                  tta=None):\n",
    "    res = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            batch = batch.to(device)\n",
    "            if tta:\n",
    "                probs = []\n",
    "                for tf in tta:\n",
    "                    aug_data = tf.augment_image(batch)\n",
    "                    out = model(aug_data)\n",
    "                    probs.append(out)\n",
    "                out = torch.stack(probs, dim=0)\n",
    "                out = out.mean(dim=0)\n",
    "            else:\n",
    "                out = model(batch)\n",
    "            labels = torch.argmax(out, dim=1).tolist()\n",
    "            res.extend(labels)\n",
    "    \n",
    "    for idx in range(len(res)):\n",
    "        res[idx] = label_mapper[res[idx]]\n",
    "    \n",
    "    subm = pd.read_csv(path_to_test_csv)\n",
    "    subm[\"label\"] = res\n",
    "    subm.to_csv(f\"{experiment_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c3e6f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_train(model,\n",
    "                 epoch_num,\n",
    "                 criterion,\n",
    "                 optimizer, \n",
    "                 train_loader,\n",
    "                 val_loader,\n",
    "                 f1,\n",
    "                 training_mode,\n",
    "                 experiment_name,\n",
    "                 log_frequency,\n",
    "                 path_to_save_weights=\"weights/\",\n",
    "                 tta=None,\n",
    "                 scheduler=None,\n",
    "                 device=device\n",
    "                ):\n",
    "    \n",
    "    train_loss = []\n",
    "    train_f1 = []\n",
    "    \n",
    "    val_loss = []\n",
    "    val_f1 = []\n",
    "    \n",
    "    best_val_score = 0\n",
    "    \n",
    "    for _ in tqdm(range(epoch_num)):\n",
    "\n",
    "        set_training(model, training_mode)\n",
    "        \n",
    "        batch_f1_train = 0\n",
    "        batch_loss_train = 0\n",
    "\n",
    "        batch_cnt = 0\n",
    "        \n",
    "        for (imgs, labels) in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            out = model(imgs)\n",
    "\n",
    "            f1_micro = f1(out, labels)\n",
    "            batch_f1_train += f1_micro\n",
    "            \n",
    "            loss = criterion(out, labels)\n",
    "            batch_loss_train += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            batch_cnt += 1\n",
    "            if batch_cnt % log_frequency == 0:\n",
    "                wandb.log({\"train micro f1\": f1_micro})\n",
    "                wandb.log({\"train loss\": loss})\n",
    "#                 print(f\"train f1 {f1_micro}, train loss {loss.item()}, batch_numer: {batch_cnt}\")\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        train_loss.append(batch_loss_train / len(train_loader))\n",
    "        train_f1.append(batch_f1_train / len(train_loader))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            batch_f1_val = 0\n",
    "            batch_loss_val = 0\n",
    "            batch_cnt = 0\n",
    "\n",
    "            for (imgs, labels) in val_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                if tta:\n",
    "                    probs = []\n",
    "                    for tf in tta:\n",
    "                        aug_data = tf.augment_image(imgs)\n",
    "                        out = model(aug_data)\n",
    "                        probs.append(out)\n",
    "                    res = torch.stack(probs, dim=0)\n",
    "                    out = res.mean(dim=0)\n",
    "                else:\n",
    "                    out = model(imgs)\n",
    "\n",
    "                f1_micro = f1(out, labels)\n",
    "                batch_f1_val += f1_micro\n",
    "\n",
    "                loss = criterion(out, labels)\n",
    "                batch_loss_val += loss.item()\n",
    "                batch_cnt += 1\n",
    "                if batch_cnt % log_frequency == 0:\n",
    "                    wandb.log({\"val micro f1\": f1_micro})\n",
    "                    wandb.log({\"val loss\": loss})\n",
    "#                     print(f\"val f1 {f1_micro}, val loss {loss.item()},  batch_numer: {batch_cnt}\")\n",
    "\n",
    "            val_loss.append(batch_loss_val / len(val_loader))\n",
    "            val_f1.append(batch_f1_val / len(val_loader))\n",
    "            # save weights\n",
    "            if val_f1[-1] >= best_val_score:\n",
    "                best_val_score = val_f1[-1]\n",
    "                torch.save(model.state_dict(), os.path.join(path_to_save_weights, f\"{experiment_name}.pth\"))\n",
    "            \n",
    "            wandb.log({\"best val micro f1\": best_val_score})\n",
    "            \n",
    "        print(f\"Train F1: {train_f1[-1]} Val F1: {val_f1[-1]}\")    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a351c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttatf = tta.Compose(\n",
    "#     [\n",
    "#         tta.HorizontalFlip(),\n",
    "#         tta.Rotate90(angles=[0, 90, 270]),\n",
    "#         tta.Multiply(factors=[0.9, 1]),        \n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# model = timm.create_model(\"convnext_small_384_in22ft1k\", pretrained=True, num_classes=30)\n",
    "# model.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(weight=torch.from_numpy(class_weights).to(device, dtype=torch.float32))\n",
    "# f1_score = F1Score(task=\"multiclass\", num_classes=30, average=\"micro\").to(device)\n",
    "# optim = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "# epoch_num = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add722f0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1935ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\" : \"beit_large_patch16_224_in22k\", \n",
    "    \"tta\" : None,\n",
    "    \"criterion\": \"weighted\", \n",
    "    \"experiment_name\": \"beit_large_patch16_224_in22k_ls\", \n",
    "    \"training_mode\" : \"\",\n",
    "    \"epoch_num\" : 30, \n",
    "    \"log_frequency\" : 10, \n",
    "    \"scheduler\" : \"step_lr\",\n",
    "    \"scheduler_step\": 3,\n",
    "    \"sheduler_gamma\": 0.1,\n",
    "    \"lr\" : 0.001, \n",
    "    \"device\" : \"cuda:0\", \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "324dbd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = config[\"experiment_name\"]\n",
    "device = config[\"device\"]\n",
    "log_frequency = config[\"log_frequency\"]\n",
    "epoch_num = config[\"epoch_num\"]\n",
    "training_mode = config[\"training_mode\"] \n",
    "lr = config[\"lr\"]\n",
    "\n",
    "model = timm.create_model(config[\"model\"], pretrained=True, num_classes=30) \n",
    "model.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "if config[\"criterion\"] == \"weighted\":\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.from_numpy(class_weights).to(device, dtype=torch.float32), \n",
    "                                    label_smoothing=0.1)\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "if config[\"scheduler\"] == \"step_lr\":\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optim, \n",
    "                                                step_size=config[\"scheduler_step\"],\n",
    "                                                gamma=config[\"sheduler_gamma\"])    \n",
    "else:\n",
    "    scheduler = None\n",
    "f1_score = F1Score(task=\"multiclass\", num_classes=30, average=\"micro\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e39e0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdmasny\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/notebooks/cv_made/wandb/run-20230417_202507-8flbo0ou</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmasny/made_cv_hw/runs/8flbo0ou' target=\"_blank\">beit_large_patch16_224_in22k_ls</a></strong> to <a href='https://wandb.ai/dmasny/made_cv_hw' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmasny/made_cv_hw' target=\"_blank\">https://wandb.ai/dmasny/made_cv_hw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmasny/made_cv_hw/runs/8flbo0ou' target=\"_blank\">https://wandb.ai/dmasny/made_cv_hw/runs/8flbo0ou</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█████▍                                                                                                                                                             | 1/30 [23:37<11:25:05, 1417.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.9331884980201721 Val F1: 0.9477311372756958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|██████████▊                                                                                                                                                        | 2/30 [47:32<11:06:22, 1427.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.9593918323516846 Val F1: 0.9502899646759033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████████████                                                                                                                                                 | 3/30 [1:11:09<10:40:19, 1422.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.9678845405578613 Val F1: 0.9500699043273926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█████████████████████▍                                                                                                                                           | 4/30 [1:34:47<10:15:42, 1420.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.9791007041931152 Val F1: 0.9510602355003357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████████████████████████                                                                                                                                       | 5/30 [1:58:27<9:51:55, 1420.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.9795757532119751 Val F1: 0.9518304467201233\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"made_cv_hw\", \n",
    "           entity=\"dmasny\",\n",
    "           name=experiment_name, \n",
    "           config=config)\n",
    "\n",
    "simple_train(model=model, \n",
    "             epoch_num=epoch_num, \n",
    "             criterion=criterion,\n",
    "             optimizer=optim,\n",
    "             train_loader=train_loader,\n",
    "             val_loader=val_loader,\n",
    "             f1=f1_score,\n",
    "             training_mode=training_mode,\n",
    "             experiment_name=experiment_name,\n",
    "             log_frequency=log_frequency,\n",
    "             path_to_save_weights=\"weights/\",\n",
    "             tta=None,\n",
    "             scheduler=scheduler,\n",
    "             device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cbfd85",
   "metadata": {},
   "source": [
    "# План экспериментов\n",
    "* stage3 block2 / stage3\n",
    "* with tta /without tta\n",
    "* epoch 10-50\n",
    "* weighted CE / standard CE\n",
    "* train augm / wo augm\n",
    "* another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "725855b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {learning_rate': lr,\n",
    "#                 'weight_decay': wd,\n",
    "#                 'epochs': epoches,\n",
    "#                 'training_batch_size' : batch_size,\n",
    "#                 'validation_batch_size' : batch_size,\n",
    "#                 'loops_config': 'allow loops',\n",
    "#                 'weight_config': 'weighted',\n",
    "#                 'split_number': split,\n",
    "#                 'criterion': criterion,    \n",
    "#                 'node_representation_size': train_dataset.num_node_features, \n",
    "#                 'activation' : 'without relu at the end',\n",
    "#                 'model': {\n",
    "#                                 'num_graph_conv_blocks': 2,\n",
    "#                                 'hidden_channels' : hidden_channels,\n",
    "#                                 'activation' : 'ReLU',\n",
    "#                                 'readout': 'global_mean_pool'}}\n",
    "\n",
    "#         wandb.init(project = 'resting_state_eeg', \n",
    "#                 entity = 'dmasny',\n",
    "#                 name = experiment_name, \n",
    "#                 config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "927ec53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Beit(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-23): 24 x Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): Identity()\n",
       "  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=1024, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = timm.create_model(\"beit_large_patch16_224_in22k\", pretrained=True, num_classes=30)\n",
    "m.load_state_dict(torch.load(\"weights/beit_large_patch16_224_in22k.pth\"))\n",
    "m.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3609caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152/152 [09:33<00:00,  3.77s/it]\n"
     ]
    }
   ],
   "source": [
    "create_submit(m,\n",
    "              test_loader,\n",
    "              idx_to_sport,\n",
    "              experiment_name=\"beit_wo_aug\",\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b306d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
